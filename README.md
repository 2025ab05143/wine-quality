# üç∑ Wine Quality Prediction Dashboard

A comprehensive machine learning web application for predicting wine quality using multiple classification algorithms. This project implements an end-to-end ML workflow with interactive data visualization and model comparison.

## üìù Problem Statement

This project implements multiple machine learning classification models to predict wine quality based on physicochemical properties. The goal is to build an interactive web application that demonstrates the complete ML workflow from data exploration to model deployment.

## üìä Dataset Description **[1 mark]**

- **Dataset:** Wine Quality Dataset (Red Wine)
- **Source:** UCI Machine Learning Repository / [GitHub Repository](https://github.com/2025ab05143/wine-quality/blob/main/winequality-red.csv)
- **Auto-Download:** The dataset is automatically downloaded from GitHub during app runtime
- **Instances:** 1,599 red wine samples
- **Features:** 11 physicochemical properties
- **Target:** Quality ratings (3-8 scale)
- **Type:** Multi-class classification problem

The dataset contains sufficient instances (>500) and features (>12 including target) as required for this assignment.

### Features:
- Fixed acidity
- Volatile acidity  
- Citric acid
- Residual sugar
- Chlorides
- Free sulfur dioxide
- Total sulfur dioxide
- Density
- pH
- Sulphates
- Alcohol
- **Quality** (target variable)

## ü§ñ Models Used **[6 marks - 1 mark for all metrics for each model]**

### 1. Logistic Regression
- Linear classification algorithm
- Good baseline performance with interpretable results
- Effective for understanding feature importance

### 2. Decision Tree Classifier
- Non-linear pattern recognition
- Rule-based interpretability
- Handles feature interactions naturally

### 3. K-Nearest Neighbors (KNN)
- Instance-based learning algorithm
- Non-parametric approach
- Effective for local pattern recognition

### 4. Gaussian Naive Bayes
- Probabilistic classifier
- Fast training and prediction
- Assumes feature independence

### 5. Random Forest (Ensemble Model)
- Bagging ensemble method
- Reduces overfitting compared to single decision tree
- Provides excellent feature importance insights

### 6. XGBoost (Ensemble Model)  
- Advanced gradient boosting algorithm
- State-of-the-art performance
- Advanced regularization techniques

## üìà Evaluation Metrics

For each model, the following 6 evaluation metrics are calculated:

| Metric | Description | Formula/Method |
|--------|-------------|----------------|
| **Accuracy** | Overall correctness of predictions | `(TP + TN) / (TP + TN + FP + FN)` |
| **AUC Score** | Area Under ROC Curve | Macro-average for multiclass |
| **Precision** | Positive predictive value | Weighted average across classes |
| **Recall** | True positive rate | Weighted average across classes |
| **F1 Score** | Harmonic mean of precision and recall | Weighted average across classes |
| **MCC Score** | Matthews Correlation Coefficient | Balanced metric for multiclass |

## üèÜ Comparison Table with Evaluation Metrics

| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |
|---------------|----------|-----|-----------|--------|----|----|
| Logistic Regression | 0.5906 | 0.7640 | 0.5695 | 0.5906 | 0.5673 | 0.3250 |
| Decision Tree Classifier | 0.6062 | 0.6574 | 0.6097 | 0.6062 | 0.6066 | 0.3944 |
| K-Nearest Neighbor Classifier | 0.6094 | 0.6983 | 0.5841 | 0.6094 | 0.5959 | 0.3733 |
| Naive Bayes Classifier | 0.5625 | 0.6838 | 0.5745 | 0.5625 | 0.5681 | 0.3299 |
| Random Forest | 0.6781 | 0.7649 | 0.6531 | 0.6781 | 0.6632 | 0.4818 |
| XGBoost | 0.6531 | 0.7990 | 0.6480 | 0.6531 | 0.6434 | 0.4453 |

## üîç Observations on Model Performance **[3 marks]**

| ML Model Name | Observation about model performance |
|---------------|-----------------------------------|
| **Logistic Regression** | Baseline performance with 59.06% accuracy but strong AUC of 0.7640. Good interpretability and reasonable performance considering the multiclass nature of wine quality prediction. MCC of 0.3250 indicates moderate correlation. |
| **Decision Tree Classifier** | Slightly better accuracy (60.62%) than logistic regression but lower AUC (0.6574), indicating some overfitting. Strong interpretability with decision rules. MCC of 0.3944 shows improved balanced performance. |
| **K-Nearest Neighbor Classifier** | Best individual accuracy (60.94%) among non-ensemble models. Good AUC of 0.6983 but sensitive to feature scaling. Computational complexity increases with data size. MCC of 0.3733 shows solid performance. |
| **Naive Bayes Classifier** | Lowest accuracy (56.25%) but reasonable AUC (0.6838). Fast training/prediction with probabilistic output. Independence assumption limits performance on correlated wine features. MCC of 0.3299 indicates moderate effectiveness. |
| **Random Forest** | **Best overall performance** with 67.81% accuracy and strong AUC of 0.7649. Excellent ensemble method that reduces overfitting. Highest MCC of 0.4818 indicates superior balanced classification performance across all quality classes. |
| **XGBoost** | Strong performance with 65.31% accuracy and **highest AUC** of 0.7990. Advanced gradient boosting handles complex feature interactions well. MCC of 0.4453 shows excellent balanced performance, making it ideal for production deployment. |

## üöÄ Streamlit App Features **[4 marks total]**

### a. Dataset Upload Option **[1 mark]**
- Interactive CSV file upload functionality
- Automatic data validation and preprocessing
- Error handling for file format and structure
- Real-time data preview and statistics

### b. Model Selection Dropdown **[1 mark]**
- Dynamic model selection interface
- Real-time model switching capabilities
- Comprehensive model comparison tools
- Interactive model parameter display

### c. Display of Evaluation Metrics **[1 mark]**
- Interactive metrics dashboard
- Real-time metric calculations
- Comparative performance visualization
- Detailed metric explanations and interpretations

### d. Confusion Matrix and Classification Report **[1 mark]**
- Interactive confusion matrix heatmaps
- Detailed per-class classification reports
- Visual performance analysis
- Exportable results and visualizations

## üõ†Ô∏è Installation and Setup

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Local Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd wine-quality-prediction
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the application**
   ```bash
   streamlit run app.py
   ```

4. **Access the application**
   - Open your browser and navigate to `http://localhost:8501`

### Streamlit Community Cloud Deployment

1. **Push to GitHub**
   - Ensure all files are committed to your GitHub repository

2. **Deploy on Streamlit Cloud**
   - Visit [share.streamlit.io](https://share.streamlit.io)
   - Connect your GitHub account
   - Select your repository
   - Deploy the app

## üìÅ Project Structure

```
project-folder/
‚îÇ
‚îú‚îÄ‚îÄ app.py                       # Main Streamlit application
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ README.md                   # Project documentation (this file)
‚îú‚îÄ‚îÄ winequality-red.csv         # Wine quality dataset
‚îú‚îÄ‚îÄ get_metrics.py              # Metrics generation script
‚îú‚îÄ‚îÄ test_models.py              # Model testing script
‚îÇ
‚îî‚îÄ‚îÄ model/                      # ML Models package (*.py files)
    ‚îú‚îÄ‚îÄ __init__.py             # Package initialization
    ‚îú‚îÄ‚îÄ model_trainer.py        # Model training utilities
    ‚îú‚îÄ‚îÄ logistic_regression.py  # Logistic Regression implementation
    ‚îú‚îÄ‚îÄ decision_tree.py        # Decision Tree implementation
    ‚îú‚îÄ‚îÄ knn.py                  # K-Nearest Neighbors implementation
    ‚îú‚îÄ‚îÄ naive_bayes.py          # Naive Bayes implementation
    ‚îú‚îÄ‚îÄ random_forest.py        # Random Forest implementation
    ‚îî‚îÄ‚îÄ xgboost_model.py        # XGBoost implementation
```

## üéØ Application Sections

### 1. üìä Dataset Overview
- Dataset statistics and information
- Feature distributions and correlations
- Data quality assessment
- Interactive data exploration

### 2. ü§ñ Model Training & Evaluation  
- Individual model performance analysis
- Detailed metrics display
- Confusion matrix visualization
- Classification reports

### 3. üìà Model Comparison
- Side-by-side model comparison
- Interactive performance charts
- Best model identification
- Comprehensive analysis

### 4. üîç Individual Predictions
- Real-time wine quality prediction
- Interactive feature input
- Model selection for prediction
- Prediction confidence display

### 5. üìã About
- Project documentation
- Technical details
- Implementation notes

## üîß Technical Implementation

### Libraries Used
- **Streamlit** - Web application framework
- **Pandas & NumPy** - Data manipulation and analysis  
- **Scikit-learn** - Machine learning algorithms and metrics
- **XGBoost** - Advanced gradient boosting
- **Plotly** - Interactive visualizations
- **Matplotlib & Seaborn** - Statistical plotting

### Key Features
- Responsive web interface
- Real-time model training and evaluation
- Interactive data visualization
- Comprehensive model comparison
- Individual prediction capability
- Professional UI/UX design

## üìä Performance Insights

### Top Performing Models
Based on comprehensive evaluation:

1. **XGBoost** - Superior performance across most metrics
2. **Random Forest** - Excellent balanced performance
3. **Logistic Regression** - Best interpretability-performance ratio

### Key Findings
- Ensemble methods consistently outperform individual algorithms
- Feature scaling significantly impacts distance-based algorithms
- Wine quality prediction benefits from non-linear approaches
- Class imbalance affects performance on extreme quality ratings

## üöÄ Live Demo

**Streamlit Community Cloud Link:** [Your deployed app URL here]

*The application provides an interactive interface for exploring the wine quality dataset and comparing different machine learning models.*

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is created for educational purposes as part of a Machine Learning assignment.

## üôè Acknowledgments

- UCI Machine Learning Repository for the Wine Quality Dataset
- Streamlit team for the excellent web framework
- Scikit-learn community for comprehensive ML tools
- XGBoost developers for advanced boosting algorithms

---

**Author:** [Your Name]  
**Course:** Machine Learning Classification Assignment  
**Technology Stack:** Python, Streamlit, Scikit-learn, XGBoost, Plotly  
**Deployment:** Streamlit Community Cloud
